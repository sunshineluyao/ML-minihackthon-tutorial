{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNFtowkMSN5mKXlvNL6AE9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunshineluyao/ML-minihackthon-tutorial/blob/main/RL_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Challenge: Reinforcement Learning for Financial Asset Optimization**\n",
        "\n",
        "#### **Objective**\n",
        "Students will design and implement a reinforcement learning (RL) model to optimize investment strategies for financial assets. The goal is to use historical market data from **Yahoo Finance** to train an RL agent that learns to maximize returns over a specified period while managing risks.\n",
        "\n",
        "---\n",
        "\n",
        "### **Activity Outline**\n",
        "\n",
        "#### **Step 1: Introduction and Setup (10 Minutes)**\n",
        "\n",
        "1. **Provide Background:**\n",
        "   - Discuss the relevance of RL in financial portfolio optimization.\n",
        "   - Explain how RL can be used to:\n",
        "     - Dynamically allocate assets.\n",
        "     - Adapt to changing market conditions.\n",
        "     - Optimize the trade-off between risk and return.\n",
        "\n",
        "2. **Dataset:**\n",
        "   - Introduce **Yahoo Finance** as the data source.\n",
        "   - Provide a Python script using `yfinance` to download historical stock prices for selected assets (e.g., S&P 500, AAPL, MSFT).\n",
        "\n",
        "3. **Environment Overview:**\n",
        "   - Explain the RL setup:\n",
        "     - **State**: Historical market data (e.g., stock prices, moving averages, or other indicators).\n",
        "     - **Actions**: Portfolio allocation percentages for each asset (e.g., 50% in stock A, 50% in stock B).\n",
        "     - **Reward**: Portfolio returns, adjusted for risk (e.g., Sharpe Ratio).\n",
        "\n",
        "4. **Install Dependencies:**\n",
        "   - Libraries: `gym`, `yfinance`, `numpy`, `pandas`, `stable-baselines3`, `matplotlib`.\n",
        "   - Provide starter code or environment template.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 2: Task Assignment (15 Minutes)**\n",
        "\n",
        "Each team will:\n",
        "1. **Select Assets**:\n",
        "   - Choose 3-5 financial assets to include in the portfolio (e.g., AAPL, TSLA, SPY, BTC-USD).\n",
        "   - Use `yfinance` to fetch daily historical prices.\n",
        "\n",
        "2. **Design the RL Environment**:\n",
        "   - Define **state space**:\n",
        "     - Input indicators such as moving averages, relative strength index (RSI), and price history.\n",
        "   - Define **action space**:\n",
        "     - Allow discrete or continuous actions representing allocation changes.\n",
        "   - Define **reward function**:\n",
        "     - Example: Use portfolio returns minus a penalty for high volatility.\n",
        "\n",
        "3. **Train the RL Model**:\n",
        "   - Implement an RL algorithm (e.g., DQN, PPO, or A2C) using `stable-baselines3`.\n",
        "   - Train the model using historical data split into training and validation sets.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 3: Implementation (45 Minutes)**\n",
        "\n",
        "1. **Data Preprocessing**:\n",
        "   - Normalize the asset prices.\n",
        "   - Compute additional features (e.g., momentum, Bollinger bands).\n",
        "\n",
        "2. **Environment Creation**:\n",
        "   - Use `gym` to create a custom financial trading environment.\n",
        "   - Example structure:\n",
        "     ```python\n",
        "     class PortfolioEnv(gym.Env):\n",
        "         def __init__(self, asset_data):\n",
        "             self.asset_data = asset_data\n",
        "             self.current_step = 0\n",
        "             self.action_space = gym.spaces.Box(low=0, high=1, shape=(len(asset_data.columns),))\n",
        "             self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(window_size, len(asset_data.columns)))\n",
        "     ```\n",
        "\n",
        "3. **Model Training**:\n",
        "   - Train the agent using algorithms like **PPO** or **DQN**.\n",
        "   - Monitor key metrics: total portfolio return, Sharpe ratio, maximum drawdown.\n",
        "\n",
        "4. **Visualization**:\n",
        "   - Plot the agent's portfolio value over time.\n",
        "   - Compare the RL agent’s performance to a baseline strategy (e.g., equal-weighted portfolio).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Step 4: Showcase and Discussion (20 Minutes)**\n",
        "\n",
        "1. **Team Presentations**:\n",
        "   - Each team presents their RL model, reward function, and results.\n",
        "   - Share visualizations of portfolio performance.\n",
        "\n",
        "2. **Class Discussion**:\n",
        "   - What challenges did teams face while integrating financial data?\n",
        "   - How did the reward function influence the agent’s behavior?\n",
        "   - What alternative strategies could improve performance (e.g., including risk-free assets)?\n",
        "\n",
        "---\n",
        "\n",
        "### **Expected Output**\n",
        "- A trained RL agent capable of making dynamic portfolio allocation decisions.\n",
        "- Visualizations comparing the agent's performance to a baseline (e.g., buy-and-hold strategy).\n",
        "- Insights into the challenges of using RL in financial markets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Starter Code**\n",
        "\n",
        "#### Data Download\n",
        "```python\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Download historical data\n",
        "assets = ['AAPL', 'MSFT', 'TSLA']\n",
        "data = yf.download(assets, start=\"2015-01-01\", end=\"2023-01-01\")['Adj Close']\n",
        "\n",
        "# Normalize prices\n",
        "normalized_data = data / data.iloc[0]\n",
        "print(normalized_data.head())\n",
        "```\n",
        "\n",
        "#### RL Environment Example\n",
        "```python\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class PortfolioEnv(gym.Env):\n",
        "    def __init__(self, data, window_size):\n",
        "        super(PortfolioEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.current_step = 0\n",
        "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(data.shape[1],))\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=np.inf, shape=(window_size, data.shape[1])\n",
        "        )\n",
        "    \n",
        "    def reset(self):\n",
        "        self.current_step = self.window_size\n",
        "        return self.data.iloc[self.current_step - self.window_size : self.current_step].values\n",
        "\n",
        "    def step(self, action):\n",
        "        portfolio_return = np.dot(action, self.data.iloc[self.current_step].pct_change())\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "        return self.data.iloc[self.current_step - self.window_size : self.current_step].values, portfolio_return, done, {}\n",
        "\n",
        "# Initialize environment\n",
        "env = PortfolioEnv(normalized_data, window_size=30)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Assessment Criteria**\n",
        "1. **Correctness**: Is the RL agent correctly set up to interact with the environment?\n",
        "2. **Performance**: Does the RL agent outperform a baseline strategy?\n",
        "3. **Innovation**: How creative are the team’s environment and reward function designs?\n",
        "4. **Clarity**: Are the results and insights well-presented?\n",
        "\n"
      ],
      "metadata": {
        "id": "s47Gq_ZWoCov"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-PeDXqj6rsWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpGph6oGoAwa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Taxonomy of Reinforcement Learning Algorithms**\n",
        "\n",
        "Reinforcement Learning (RL) algorithms can be categorized based on their learning paradigm, type of feedback, and whether they model the environment explicitly. Below is a taxonomy of RL algorithms with key mathematical formulas to illustrate their differences.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Value-Based Methods**\n",
        "\n",
        "Value-based methods focus on learning the value function, which estimates how good it is for an agent to be in a given state or take a particular action in a state.\n",
        "\n",
        "#### **1.1 Q-Learning**\n",
        "- **Objective**: Learn the optimal action-value function $Q^*(s, a)$.\n",
        "- **Update Rule**:\n",
        "  $$\n",
        "  Q(s_t, a_t) \\gets Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right]\n",
        "  $$\n",
        "  where:\n",
        "  - $\\alpha$: Learning rate\n",
        "  - $\\gamma$: Discount factor\n",
        "  - $r_{t+1}$: Reward\n",
        "  - $\\max_{a'} Q(s_{t+1}, a')$: Maximum value of the next state\n",
        "\n",
        "#### **1.2 Deep Q-Learning (DQN)**\n",
        "- Extends Q-Learning by approximating $Q(s, a)$ with a neural network $Q(s, a; \\theta)$, where $\\theta$ are the network weights.\n",
        "- **Loss Function**:\n",
        "  $$\n",
        "  L(\\theta) = \\mathbb{E}_{(s, a, r, s')} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right]\n",
        "  $$\n",
        "  where $\\theta^-$ are the target network weights.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Policy-Based Methods**\n",
        "\n",
        "Policy-based methods directly learn a policy $\\pi(a|s)$, which maps states to actions, without explicitly learning a value function.\n",
        "\n",
        "#### **2.1 REINFORCE**\n",
        "- A Monte Carlo policy gradient method that maximizes the expected return.\n",
        "- **Objective**:\n",
        "  $$\n",
        "  J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t r_t \\right]\n",
        "  $$\n",
        "- **Gradient Update**:\n",
        "  $$\n",
        "  \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) R_t \\right]\n",
        "  $$\n",
        "  where $R_t$ is the cumulative reward from time step $t$.\n",
        "\n",
        "#### **2.2 Proximal Policy Optimization (PPO)**\n",
        "- Introduces a clipped objective to improve stability.\n",
        "- **Clipped Objective**:\n",
        "  $$\n",
        "  L^{\\text{PPO}}(\\theta) = \\mathbb{E}_t \\left[ \\min\\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]\n",
        "  $$\n",
        "  where:\n",
        "  - $r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$: Probability ratio\n",
        "  - $A_t$: Advantage function\n",
        "  - $\\epsilon$: Clipping threshold\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Actor-Critic Methods**\n",
        "\n",
        "Actor-Critic methods combine policy-based and value-based approaches by learning both a policy (actor) and a value function (critic).\n",
        "\n",
        "#### **3.1 Advantage Actor-Critic (A2C)**\n",
        "- **Actor Loss**:\n",
        "  $$\n",
        "  L^{\\text{actor}} = -\\mathbb{E}_t \\left[ \\log \\pi_\\theta(a_t | s_t) A_t \\right]\n",
        "  $$\n",
        "- **Critic Loss**:\n",
        "  $$\n",
        "  L^{\\text{critic}} = \\mathbb{E}_t \\left[ \\left( r_t + \\gamma V(s_{t+1}) - V(s_t) \\right)^2 \\right]\n",
        "  $$\n",
        "\n",
        "#### **3.2 Deep Deterministic Policy Gradient (DDPG)**\n",
        "- Suitable for continuous action spaces.\n",
        "- **Actor Update**:\n",
        "  $$\n",
        "  \\nabla_\\theta J = \\mathbb{E}_t \\left[ \\nabla_a Q(s_t, a_t | \\phi) \\nabla_\\theta \\mu_\\theta(s_t) \\right]\n",
        "  $$\n",
        "- **Critic Update**:\n",
        "  $$\n",
        "  L(\\phi) = \\mathbb{E}_{(s, a, r, s')} \\left[ \\left( r + \\gamma Q(s', \\mu_\\theta(s') | \\phi) - Q(s, a | \\phi) \\right)^2 \\right]\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Model-Based Methods**\n",
        "\n",
        "Model-based methods learn a model of the environment’s dynamics $P(s'|s, a)$ and/or $R(s, a)$ to simulate future trajectories.\n",
        "\n",
        "#### **4.1 Model Predictive Control (MPC)**\n",
        "- Uses the learned model to optimize actions over a planning horizon $H$.\n",
        "- **Optimization Objective**:\n",
        "  $$\n",
        "  \\max_{a_0, \\dots, a_{H-1}} \\mathbb{E} \\left[ \\sum_{t=0}^{H-1} \\gamma^t R(s_t, a_t) \\right]\n",
        "  $$\n",
        "\n",
        "#### **4.2 AlphaZero**\n",
        "- Combines Monte Carlo Tree Search (MCTS) with neural networks for model-based learning.\n",
        "- **Policy and Value Updates**:\n",
        "  $$\n",
        "  L = \\mathbb{E}_{(s, \\pi, z)} \\left[ -\\pi^T \\log p + (z - v)^2 \\right]\n",
        "  $$\n",
        "  where:\n",
        "  - $\\pi$: Target policy from MCTS\n",
        "  - $z$: True outcome of the game\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of Key Categories**\n",
        "\n",
        "| **Category**      | **Examples**           | **Key Feature**                                                                 | **Primary Formula**                                                                                                    |\n",
        "|--------------------|------------------------|--------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Value-Based**    | Q-Learning, DQN        | Learns value functions to select optimal actions.                              | $ Q(s, a) \\gets Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] $                                         |\n",
        "| **Policy-Based**   | REINFORCE, PPO         | Directly learns policies to map states to actions.                             | $ \\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) R] $                                       |\n",
        "| **Actor-Critic**   | A2C, DDPG              | Combines policy (actor) and value (critic) learning.                           | $ L^{\\text{actor}} = -\\mathbb{E}[\\log \\pi_\\theta(a|s) A] $ and $ L^{\\text{critic}} = \\mathbb{E}[(r + \\gamma V(s') - V(s))^2] $ |\n",
        "| **Model-Based**    | MPC, AlphaZero         | Uses a learned model of the environment to plan and predict future states.     | $ \\max_{a_0, \\dots, a_H} \\mathbb{E}[\\sum_{t=0}^{H-1} \\gamma^t R(s_t, a_t)] $                                          |\n"
      ],
      "metadata": {
        "id": "vnf6MNfYrsub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conventional Evaluation Criteria for Reinforcement Learning Algorithms**\n",
        "\n",
        "Reinforcement learning (RL) algorithms are typically evaluated on their ability to learn and perform tasks effectively under specified conditions. Here are the key evaluation criteria:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Learning Efficiency**\n",
        "- **Definition**: Measures how quickly an RL algorithm converges to an optimal policy.\n",
        "- **Key Metrics**:\n",
        "  - **Cumulative Reward**: Sum of rewards accumulated during training. Faster convergence to high rewards indicates better efficiency.\n",
        "    $$\n",
        "    G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\n",
        "    $$\n",
        "    where $G_t$ is the discounted return, $\\gamma$ is the discount factor, and $r_{t+k}$ is the reward at step $t+k$.\n",
        "\n",
        "  - **Episodes to Convergence**: Number of episodes required for the algorithm to achieve near-optimal performance.\n",
        "  - **Sample Efficiency**: Number of interactions with the environment required to reach optimal performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Policy Quality**\n",
        "- **Definition**: Measures the quality of the policy learned by the agent.\n",
        "- **Key Metrics**:\n",
        "  - **Average Reward per Episode**:\n",
        "    $$\n",
        "    R_{\\text{avg}} = \\frac{\\sum_{t=0}^{T} r_t}{T}\n",
        "    $$\n",
        "    where $T$ is the total time steps or episodes.\n",
        "  - **Optimality Gap**: Difference between the agent's achieved performance and the theoretical optimal policy performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Generalization**\n",
        "- **Definition**: Evaluates the algorithm's ability to adapt to unseen states or tasks not explicitly encountered during training.\n",
        "- **Key Approaches**:\n",
        "  - Testing on **perturbed environments** (e.g., different initial conditions, obstacles, or dynamics).\n",
        "  - Introducing stochasticity and evaluating robustness.\n",
        "  - **Metric**: Reward in a test environment different from the training environment.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Robustness**\n",
        "- **Definition**: Ability to handle noise, delays, or variations in the environment's dynamics.\n",
        "- **Key Evaluations**:\n",
        "  - Adding noise to state observations or actions.\n",
        "  - Simulating hardware failures or partial observability.\n",
        "  - **Metric**: Performance degradation compared to the noise-free environment.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Stability**\n",
        "- **Definition**: Stability assesses whether the agent avoids oscillations or catastrophic forgetting during training.\n",
        "- **Key Metrics**:\n",
        "  - **Variance in Rewards**:\n",
        "    $$\n",
        "    \\text{Variance} = \\frac{\\sum_{i=1}^{N} (R_i - \\bar{R})^2}{N}\n",
        "    $$\n",
        "    where $R_i$ is the reward in episode $i$, and $\\bar{R}$ is the mean reward.\n",
        "  - **Policy Stability**: How much the policy changes after consecutive updates.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Scalability**\n",
        "- **Definition**: The ability of the algorithm to handle larger state-action spaces or more complex environments.\n",
        "- **Key Considerations**:\n",
        "  - Training time as the environment grows.\n",
        "  - Memory usage and computational complexity.\n",
        "  - Performance in high-dimensional or multi-agent tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Fairness and Interpretability**\n",
        "- **Fairness**: Does the agent treat similar states or scenarios equitably?\n",
        "  - Measured through policy consistency or fairness metrics (e.g., equal cumulative rewards for similar states).\n",
        "- **Interpretability**: Ability to explain why certain actions are chosen.\n",
        "  - Techniques: Saliency maps, policy heatmaps, or rule-based approximations.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Safety**\n",
        "- **Definition**: Ensures the agent avoids undesirable or unsafe actions during learning and deployment.\n",
        "- **Key Metrics**:\n",
        "  - **Constraint Violation Rate**: Frequency of actions that violate constraints.\n",
        "  - **Cost-Aware Reward**:\n",
        "    $$\n",
        "    G_t = \\sum_{k=0}^{\\infty} \\gamma^k (r_{t+k} - c_{t+k})\n",
        "    $$\n",
        "    where $c_{t+k}$ is the cost associated with unsafe actions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Theoretical Evaluation Criteria**\n",
        "\n",
        "To evaluate RL algorithms in theory:\n",
        "1. **Convergence Guarantees**:\n",
        "   - Does the algorithm converge to the optimal policy under specific assumptions?\n",
        "   - Example: Q-Learning is proven to converge to $Q^*$ under the assumptions of infinite exploration and a decaying learning rate.\n",
        "\n",
        "2. **Complexity Analysis**:\n",
        "   - Computational complexity: Time and memory required to compute updates.\n",
        "   - Sample complexity: Number of environment interactions needed to learn an optimal policy.\n",
        "\n",
        "3. **Bounded Optimality**:\n",
        "   - Performance bounds for suboptimal policies. For example, Approximate Policy Iteration guarantees that the policy's value function is within:\n",
        "     $$\n",
        "     \\frac{2\\gamma \\epsilon}{(1 - \\gamma)^2}\n",
        "     $$\n",
        "     of the optimal value.\n",
        "\n",
        "4. **Robustness Bounds**:\n",
        "   - Analyze the sensitivity of the policy to errors in state transitions or reward functions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison with Data and Benchmarks**\n",
        "\n",
        "1. **Role of Data and Benchmarks**:\n",
        "   - Benchmarks standardize evaluations by providing well-defined environments and datasets for testing.\n",
        "   - Examples: OpenAI Gym, Mujoco, CARLA, or custom datasets (e.g., Yahoo Finance for financial applications).\n",
        "\n",
        "2. **Evaluation Criteria with Data**:\n",
        "   - Use benchmarks to compare **empirical performance** (e.g., reward, robustness) across algorithms under identical conditions.\n",
        "   - Example Metrics:\n",
        "     - Average cumulative reward on CartPole, Mujoco, or CARLA.\n",
        "     - Number of collisions or lane deviations in CARLA for autonomous driving.\n",
        "\n",
        "3. **Empirical vs. Theoretical Evaluation**:\n",
        "   - **Theoretical Evaluation**: Provides guarantees under idealized conditions, useful for algorithm design and analysis.\n",
        "   - **Empirical Evaluation**: Tests algorithms in practice, revealing performance under real-world conditions (e.g., stochastic environments).\n",
        "\n",
        "4. **Example Framework**:\n",
        "   - **Theoretical Analysis**: Demonstrate convergence and efficiency on small-scale problems (e.g., gridworld).\n",
        "   - **Empirical Validation**: Test on benchmarks with complex dynamics (e.g., Mujoco or RoboSumo).\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary Table: Evaluation Framework**\n",
        "\n",
        "| **Criterion**       | **Theoretical Evaluation**                                  | **Empirical Evaluation**                                                 |\n",
        "|----------------------|------------------------------------------------------------|---------------------------------------------------------------------------|\n",
        "| **Learning Efficiency** | Convergence proofs, sample complexity bounds               | Cumulative reward, episodes to convergence                               |\n",
        "| **Policy Quality**   | Approximation error bounds, bounded optimality              | Average reward, performance in benchmarks                                |\n",
        "| **Generalization**   | Robustness to changes in state transitions                  | Test on unseen environments or perturbed conditions                      |\n",
        "| **Robustness**       | Error sensitivity bounds                                    | Performance under noise or partial observability                         |\n",
        "| **Scalability**      | Time and space complexity                                   | Performance in large or high-dimensional environments                    |\n",
        "| **Fairness**         | Analytical fairness guarantees                              | Equitable performance across different scenarios                         |\n",
        "| **Safety**           | Constraint violation proofs                                 | Rate of unsafe actions, cost-aware reward                                |\n",
        "| **Stability**        | Stability of policy updates                                 | Variance in rewards or policy stability across iterations                |\n",
        "\n"
      ],
      "metadata": {
        "id": "iZ52VDlJq1-M"
      }
    }
  ]
}